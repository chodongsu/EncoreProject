{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naver_crawling.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN77wFRQ96EQzGHO0NwdU96",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChoSanghyuk/EncoreProject/blob/master/Naver_crawling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjSi1b642etD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "1731ebd9-77b8-40e1-c33e-3ea5df7bdfa0"
      },
      "source": [
        "from selenium import webdriver\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cb572f4dc8c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT6VteQD20vY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 조건에 맞는 네이버 기사만 스크랩\n",
        "def crawler(maxpage,query,s_date,e_date):\n",
        "    s_from = s_date.replace(\".\",\"\")     # url에 들어갈 시작 날짜\n",
        "    e_to = e_date.replace(\".\",\"\")       # url에 들어갈 끝 날짜\n",
        "    page = 1\n",
        "    maxpage_t =(int(maxpage)-1)*10+1    # 네이버 형식에 맞게 수치변환 11= 2페이지 21=3페이지 31=4페이지 ...81=9페이지 , 91=10페이지, 101=11페이지\n",
        "    f = open(\"C:/Users/Playdata/Documents/moonju/contents_text.csv\", 'w', encoding='utf-8')     # 결과 저장할 파일 설정\n",
        "    while page < maxpage_t:             \n",
        "        print(page)\n",
        "        url = \"https://search.naver.com/search.naver?where=news&query=\" + query + \"&sort=0&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
        "        req = requests.get(url)         # url 주소 페이지 가져옴\n",
        "        #print(url)\n",
        "        cont = req.content              \n",
        "        soup = BeautifulSoup(cont, 'html.parser')   # html 파싱\n",
        "        #print(soup)\n",
        "        for urls in soup.select(\"._sp_each_url\"):   # 여러 언론사중 네이버 기사만 크롬창을 띄워 데이터 확인\n",
        "            try :\n",
        "                #print(urls[\"href\"])\n",
        "                if urls[\"href\"].startswith(\"https://news.naver.com\"):\n",
        "                    #print(urls[\"href\"])\n",
        "                    news_detail = get_news(f,urls[\"href\"])  # 크롬창에 기사 띄워 이후 작업\n",
        "            except Exception as e:                  # 크롤링 도중 에러 시 에러 메시지 출력\n",
        "                print(e)\n",
        "                continue\n",
        "        page += 10\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyOwNNz521Mn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_news(f,url):\n",
        "\n",
        "    #웹 드라이버 (캐시데이터 등을 초기화하기 위해 크롬 창을 띄움)\n",
        "    driver = webdriver.Chrome('./chromedriver.exe')\n",
        "    driver.implicitly_wait(5)   # 크롬창을 띄울때 시간차를 줘 오류 방지\n",
        "    driver.get(url)\n",
        "\n",
        "    # 클린봇 옵션 해제 후 추출(잘 안됨)\n",
        "    # cleanbot  = driver.find_element_by_css_selector('a.u_cbox_cleanbot_setbutton')\n",
        "    # cleanbot.click()\n",
        "    # time.sleep(1)\n",
        "    # cleanbot_disable = driver.find_element_by_xpath(\"//input[@id='cleanbot_dialog_checkbox_cbox_module']\")\n",
        "    # cleanbot_disable.click()\n",
        "    # time.sleep(1)\n",
        "    # cleanbot_confirm = driver.find_element_by_css_selector('div.u_cbox_layer_cleanbot_extrabutton')\n",
        "    # cleanbot_confirm.click()\n",
        "    # time.sleep(1)\n",
        "\n",
        "    #더보기 계속 클릭하기\n",
        "    while True:\n",
        "        try:\n",
        "            btn_more = driver.find_element_by_css_selector('a.u_cbox_btn_more') # 더보기 태그 \n",
        "            btn_more.click()        # 클릭 액션\n",
        "            # time.sleep(1)\n",
        "        except:\n",
        "            break\n",
        "\n",
        "    #날짜 추출\n",
        "    news_date = driver.find_element_by_css_selector('span.t11') # 해당 태그 하나만 선택해 추출\n",
        "    news_date = news_date.text[:11]         # 날짜 슬라이싱\n",
        "\n",
        "    #기사제목 추출\n",
        "    article_head = driver.find_elements_by_css_selector('#articleTitle')\n",
        "    # print(\"기사 제목 : \" + article_head[0].text)\n",
        "\n",
        "    # 좋아요 수 추출\n",
        "    like_count = driver.find_elements_by_css_selector('em.u_cbox_cnt_recomm ')  # 더보기 펼쳐진 상태에서 좋아요 수 전부 추출하여 리스트 형식으로 저장\n",
        "\n",
        "    # 남여 성비와 연령대 추출 (정상 작동)\n",
        "    # per = driver.find_elements_by_css_selector('span.u_cbox_chart_per')\n",
        "    # print(\"남자 성비 : \" + per[0].text)\n",
        "    # print(\"여자 성비 : \" + per[1].text)\n",
        "    # print(\"10대 : \" + per[2].text)\n",
        "    # print(\"20대 : \" + per[3].text)\n",
        "    # print(\"30대 : \" + per[4].text)\n",
        "    # print(\"40대 : \" + per[5].text)\n",
        "    # print(\"50대 : \" + per[6].text)\n",
        "    # print(\"60대 이상 : \" + per[7].text)\n",
        "\n",
        "    # 댓글추출\n",
        "    contents = driver.find_elements_by_css_selector('span.u_cbox_contents') # 더보기로 펼쳐진 댓글 전부 추출하여 리스트 형식으로 저장\n",
        "    # for content in contents:\n",
        "    #     print(content.text)\n",
        "\n",
        "    #좋아요 수 10개 미만 제거\n",
        "    like = []\n",
        "    cont = []\n",
        "    for i in range(len(like_count)):\n",
        "        if int(like_count[i].text) >= 10 :  # 좋아요 수가 10 이상인 것들 선택\n",
        "            like.append(like_count[i].text) # 좋아요 데이터를 새로운 리스트에 삽입\n",
        "            cont.append(contents[i].text)   # 댓글 데이터 새로운 리스트에 삽입\n",
        "\n",
        "    for i in range(0, len(like)):           # 날짜, 제목, 댓글, 좋아요 순으로 csv 파일에 write\n",
        "        f.write(\"{}, {}, {}, {}\\n\".format(news_date, article_head[0].text, cont[i], like[i]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ywqnAsD21Ux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    maxpage = \"2\"               # 검색할 페이지 수\n",
        "    query = \"공공 의대 확대\"    # 키워드\n",
        "    s_date = \"2020.08.07\"       # 검색 시작 날짜\n",
        "    e_date = \"2020.08.09\"       # 검색 끝 날짜\n",
        "    crawler(maxpage,query,s_date,e_date) # 네이버 뉴스 검색 창에서 크롤링 시작"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2UkpS5Y21aw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}